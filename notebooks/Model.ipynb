{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/dlarionov/feature-engineering-xgboost\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from itertools import product\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "#import seaborn as sns\n",
    "#import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "from xgboost import plot_importance\n",
    "\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, STATUS_FAIL, Trials, space_eval\n",
    "\n",
    "import time\n",
    "import sys\n",
    "import gc\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_features(booster, figsize):    \n",
    "    fig, ax = plt.subplots(1,1,figsize=figsize)\n",
    "    return plot_importance(booster=booster, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_pickle('../data/data.pkl')\n",
    "data = pd.read_pickle('../data/test.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features through trial and error\n",
    "data = data[[\n",
    "    'date_block_num',\n",
    "    'shop_id',\n",
    "    'item_id',\n",
    "    'item_cnt_month',\n",
    "    'city_code',\n",
    "    'item_category_id',\n",
    "    'type_code',\n",
    "    'subtype_code',\n",
    "    'item_cnt_month_lag_1',\n",
    "    'item_cnt_month_lag_2',\n",
    "    'item_cnt_month_lag_3',\n",
    "    'item_cnt_month_lag_6',\n",
    "    'item_cnt_month_lag_12',\n",
    "    'date_avg_item_cnt_lag_1',\n",
    "    'date_item_avg_item_cnt_lag_1',\n",
    "    'date_item_avg_item_cnt_lag_2',\n",
    "    'date_item_avg_item_cnt_lag_3',\n",
    "    'date_item_avg_item_cnt_lag_6',\n",
    "    'date_item_avg_item_cnt_lag_12',\n",
    "    'date_shop_avg_item_cnt_lag_1',\n",
    "    'date_shop_avg_item_cnt_lag_2',\n",
    "    'date_shop_avg_item_cnt_lag_3',\n",
    "    'date_shop_avg_item_cnt_lag_6',\n",
    "    'date_shop_avg_item_cnt_lag_12',\n",
    "    'date_cat_avg_item_cnt_lag_1',\n",
    "    'date_shop_cat_avg_item_cnt_lag_1',\n",
    "    'date_city_avg_item_cnt_lag_1',\n",
    "    'date_item_city_avg_item_cnt_lag_1',\n",
    "    'delta_price_lag',\n",
    "    'month',\n",
    "    'days',\n",
    "    'item_shop_last_sale',\n",
    "    'item_last_sale',\n",
    "    'item_shop_first_sale',\n",
    "    'item_first_sale',\n",
    "]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation strategy is 34 month for the test set,\n",
    "#33 month for the validation set and 13-33 months for the train.\n",
    "X_train = data[data.date_block_num < 33].drop(['item_cnt_month'], axis=1)\n",
    "Y_train = data[data.date_block_num < 33]['item_cnt_month']\n",
    "\n",
    "X_valid = data[data.date_block_num == 33].drop(['item_cnt_month'], axis=1)\n",
    "Y_valid = data[data.date_block_num == 33]['item_cnt_month']\n",
    "\n",
    "X_test = data[data.date_block_num == 34].drop(['item_cnt_month'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del data\n",
    "gc.collect();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use XGBoost and hyperopt\n",
    "# https://www.analyticsvidhya.com/blog/2016/02/complete-guide-parameter-tuning-gradient-boosting-gbm-python/\n",
    "# https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.sklearn\n",
    "# https://github.com/dmlc/xgboost/blob/master/doc/parameter.rst#parameters-for-tree-booster\n",
    "def score(params):\n",
    "    model = XGBRegressor(**params)\n",
    "    \n",
    "    model.fit(X_train, Y_train, eval_set=[(X_train, Y_train), (X_valid, Y_valid)],\n",
    "              verbose=False, early_stopping_rounds=10)\n",
    "    Y_pred = model.predict(X_valid).clip(0, 20)\n",
    "    score = sqrt(mean_squared_error(Y_valid, Y_pred))\n",
    "    print(score)\n",
    "    return {'loss': score, 'status': STATUS_OK}    \n",
    "    \n",
    "def optimize(trials):\n",
    "    space = {\n",
    "        'max_depth':hp.choice('max_depth', np.arange(10, 25, 1, dtype=int)),\n",
    "        'n_estimators':hp.choice('n_estimators', np.arange(1000, 10000, 10, dtype=int)),\n",
    "        'colsample_bytree':hp.quniform('colsample_bytree', 0.5, 1.0, 0.1),\n",
    "        'min_child_weight':hp.choice('max_depth', np.arange(250, 350, 10, dtype=int)),\n",
    "        'subsample':hp.quniform('colsample_bytree', 0.7, 0.9, 0.1),\n",
    "        'eta':hp.quniform('eta', 0.1, 0.3, 0.1),\n",
    "        \n",
    "        'objective':'reg:squarederror',\n",
    "        \n",
    "        'tree_method':'gpu_hist',\n",
    "        'eval_metric': 'rmse'\n",
    "    }\n",
    "    \n",
    "    best = fmin(score, space, algo=tpe.suggest, max_evals=10)\n",
    "    return best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = Trials()\n",
    "best_params = optimize(trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return the best parameters\n",
    "space = {\n",
    "        'max_depth':hp.choice('max_depth', np.arange(10, 25, 1, dtype=int)),\n",
    "        'n_estimators':hp.choice('n_estimators', np.arange(1000, 10000, 10, dtype=int)),\n",
    "        'colsample_bytree':hp.quniform('colsample_bytree', 0.5, 1.0, 0.1),\n",
    "        'min_child_weight':hp.choice('max_depth', np.arange(250, 350, 10, dtype=int)),\n",
    "        'subsample':hp.quniform('colsample_bytree', 0.7, 0.9, 0.1),\n",
    "        'eta':hp.quniform('eta', 0.1, 0.3, 0.1),\n",
    "        \n",
    "        'objective':'reg:squarederror',\n",
    "        \n",
    "        'tree_method':'gpu_hist',\n",
    "        'eval_metric': 'rmse',\n",
    "    }\n",
    "space_eval(space, best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model with the best parameters\n",
    "model = XGBRegressor(\n",
    "    max_depth=22,\n",
    "    n_estimators=6830,\n",
    "    colsample_bytree=0.5,\n",
    "    min_child_weight=300,\n",
    "    subsample=0.8,    \n",
    "    eta=0.1,\n",
    "    objective='reg:squarederror',\n",
    "    tree_method='gpu_hist')\n",
    "\n",
    "model.fit(\n",
    "    X_train, \n",
    "    Y_train, \n",
    "    eval_metric=\"rmse\", \n",
    "    eval_set=[(X_train, Y_train), (X_valid, Y_valid)], \n",
    "    verbose=True, \n",
    "    early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "plot_features(model, (10, 14))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction\n",
    "Y_pred = model.predict(X_valid).clip(0, 20)\n",
    "Y_test = model.predict(X_test).clip(0, 20)\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    \"ID\": test.index, \n",
    "    \"item_cnt_month\": Y_test\n",
    "})\n",
    "submission.to_csv('../submission/xgb_submission.csv', index=False)\n",
    "\n",
    "# save predictions for an ensemble\n",
    "pickle.dump(Y_pred, open('../submission/xgb_train.pickle', 'wb'))\n",
    "pickle.dump(Y_test, open('../submission/xgb_test.pickle', 'wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
